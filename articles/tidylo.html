<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="tidylo">
<title>Tidy Log Odds • tidylo</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.0/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.0/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Tidy Log Odds">
<meta property="og:description" content="tidylo">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">tidylo</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item">
  <a class="nav-link" href="../articles/tidylo.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="http://github.com/juliasilge/tidylo/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Tidy Log Odds</h1>
                        <h4 data-toc-skip class="author">Julia Silge and Tyler Schnoebelen</h4>
            
            <h4 data-toc-skip class="date">2022-03-22</h4>
      
      <small class="dont-index">Source: <a href="http://github.com/juliasilge/tidylo/blob/HEAD/vignettes/tidylo.Rmd" class="external-link"><code>vignettes/tidylo.Rmd</code></a></small>
      <div class="d-none name"><code>tidylo.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="a-motivating-example-what-words-are-important-to-a-text">A motivating example: what words are important to a text?<a class="anchor" aria-label="anchor" href="#a-motivating-example-what-words-are-important-to-a-text"></a>
</h2>
<p>There are multiple ways to measure which words (or bigrams, or other units of text) are important in a text. You can count words, or <a href="https://www.tidytextmining.com/tfidf.html" class="external-link">measure tf-idf</a>. This package implements a different approach for measuring which words are important to a text, a <strong>weighted log odds</strong>.</p>
<p>A log odds ratio is a way of expressing probabilities, and we can weight a log odds ratio so that our implementation does a better job dealing with different combinations of words and documents having different counts. In particular, we use the method outlined in <a href="https://doi.org/10.1093/pan/mpn018" class="external-link">Monroe, Colaresi, and Quinn (2008)</a> for posterior log odds ratios, assuming a multinomial model with a Dirichlet prior. The default prior is <strong>estimated from the data itself</strong>, an empirical Bayesian approach, but an uninformative prior is also available.</p>
<p>What does this mean? It means that by weighting using empirical Bayes estimation, we take into account the sampling error in our measurements and acknowledge that we are more certain when we’ve counted something a lot of times and less certain when we’ve counted something only a few times. When weighting by a prior in this way, we focus on differences that are more likely to be real, given the evidence that we have.</p>
<p>Let’s look at just such an example.</p>
</div>
<div class="section level2">
<h2 id="jane-austen-and-bigrams">Jane Austen and bigrams<a class="anchor" aria-label="anchor" href="#jane-austen-and-bigrams"></a>
</h2>
<p>Let’s explore the <a href="https://github.com/juliasilge/janeaustenr" class="external-link">six published, completed novels of Jane Austen</a> and use the <a href="https://github.com/juliasilge/tidytext" class="external-link">tidytext</a> package to count up the bigrams (sequences of two adjacent words) in each novel, focusing on bigrams that include <em>no upper case letters</em>. (We can look at differences other than those involving proper nouns this way.) This weighted log odds approach would work equally well for single words, or other kinds of n-grams.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/juliasilge/janeaustenr" class="external-link">janeaustenr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/juliasilge/tidytext" class="external-link">tidytext</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="http://stringr.tidyverse.org" class="external-link">stringr</a></span><span class="op">)</span>

<span class="va">tidy_bigrams</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/janeaustenr/man/austen_books.html" class="external-link">austen_books</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/tidytext/man/unnest_tokens.html" class="external-link">unnest_tokens</a></span><span class="op">(</span><span class="va">bigram</span>, <span class="va">text</span>, token<span class="op">=</span><span class="st">"ngrams"</span>, n <span class="op">=</span> <span class="fl">2</span>, to_lower <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://stringr.tidyverse.org/reference/str_detect.html" class="external-link">str_detect</a></span><span class="op">(</span><span class="va">bigram</span>, <span class="st">"[A-Z]"</span><span class="op">)</span><span class="op">)</span>

<span class="va">bigram_counts</span> <span class="op">&lt;-</span> <span class="va">tidy_bigrams</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">book</span>, <span class="va">bigram</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">bigram_counts</span>
<span class="co">#&gt; <span style="color: #949494;"># A tibble: 256,348 × 3</span></span>
<span class="co">#&gt;    book                bigram     n</span>
<span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span>               <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;int&gt;</span></span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> Mansfield Park      of the   708</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> Mansfield Park      to be    582</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> Emma                to be    574</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> Emma                of the   524</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> Mansfield Park      in the   520</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> Pride &amp; Prejudice   of the   437</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> Pride &amp; Prejudice   to be    416</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> Sense &amp; Sensibility to be    410</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> Persuasion          of the   409</span>
<span class="co">#&gt; <span style="color: #BCBCBC;">10</span> Emma                in the   405</span>
<span class="co">#&gt; <span style="color: #949494;"># … with 256,338 more rows</span></span></code></pre></div>
<p>Notice that we haven’t removed stop words, or filtered out rarely used words. We have done very little pre-processing of this text data.</p>
<p>Now let’s use the <code><a href="../reference/bind_log_odds.html">bind_log_odds()</a></code> function from the tidylo package to find the weighted log odds for each bigram. The weighted log odds computed by this function are also <a href="https://en.wikipedia.org/wiki/Standard_score" class="external-link">z-scores</a> for the log odds; this quantity is useful for comparing frequencies across categories or sets but its relationship to an odds ratio is not straightforward after the weighting.</p>
<p>What are the bigrams with the highest weighted log odds for these books?</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/juliasilge/tidylo" class="external-link">tidylo</a></span><span class="op">)</span>

<span class="va">bigram_log_odds</span> <span class="op">&lt;-</span> <span class="va">bigram_counts</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="../reference/bind_log_odds.html">bind_log_odds</a></span><span class="op">(</span><span class="va">book</span>, <span class="va">bigram</span>, <span class="va">n</span><span class="op">)</span> 

<span class="va">bigram_log_odds</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/arrange.html" class="external-link">arrange</a></span><span class="op">(</span><span class="op">-</span><span class="va">log_odds_weighted</span><span class="op">)</span>
<span class="co">#&gt; <span style="color: #949494;"># A tibble: 256,348 × 4</span></span>
<span class="co">#&gt;    book                bigram            n log_odds_weighted</span>
<span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span>               <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>         <span style="color: #949494; font-style: italic;">&lt;int&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> Emma                any thing       150             13.3 </span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> Northanger Abbey    pump room        23              9.32</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> Sense &amp; Sensibility any thing        58              9.17</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> Northanger Abbey    the pump         22              9.11</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> Northanger Abbey    the abbey        20              8.69</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> Northanger Abbey    the general's    15              7.53</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> Persuasion          any thing         5              7.36</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> Emma                any body         61              6.72</span>
<span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> Emma                every body       67              6.61</span>
<span class="co">#&gt; <span style="color: #BCBCBC;">10</span> Sense &amp; Sensibility the cottage      26              6.28</span>
<span class="co">#&gt; <span style="color: #949494;"># … with 256,338 more rows</span></span></code></pre></div>
<p>The highest log odds bigrams (bigrams more likely to come from each book, compared to the others) involve concepts specific to each novel, such as the pump room in <em>Northanger Abbey</em> and sister/mother figures in <em>Sense &amp; Sensibility</em>. We can make a visualization as well.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span>

<span class="va">bigram_log_odds</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html" class="external-link">group_by</a></span><span class="op">(</span><span class="va">book</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html" class="external-link">slice_max</a></span><span class="op">(</span><span class="va">log_odds_weighted</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="va">ungroup</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>bigram <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/reorder.factor.html" class="external-link">reorder</a></span><span class="op">(</span><span class="va">bigram</span>, <span class="va">log_odds_weighted</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">log_odds_weighted</span>, <span class="va">bigram</span>, fill <span class="op">=</span> <span class="va">book</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_col</a></span><span class="op">(</span>show.legend <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">facet_wrap</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/vars.html" class="external-link">vars</a></span><span class="op">(</span><span class="va">book</span><span class="op">)</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>y <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></code></pre></div>
<p><img src="tidylo_files/figure-html/bigram_plot-1.png" width="960"></p>
<p>These bigrams have the highest weighted log odds for each book.</p>
<p>The proper names of characters tend to be unique from book to book unless you’re looking at a series like <em>Harry Potter</em>. When it comes identifying words or phrases that are unique to a text, a measure like tf-idf does a good job. But because of the way tf-idf is calculated, it cannot distinguish between words that are used in <strong>all</strong> texts.</p>
<p>For example, the phrase “had been” is used over 100 times in all six Austen novels. The “idf” in tf-idf stands for “inverse document frequency”. When a word is in all the texts, tf-idf is zero for all of them. In weighted log odds, however, each book has a different value; in this case the weighted log odds ranges from 5.42 in <em>Persuasion</em> (very high) down to -2.27 in <em>Sense &amp; Sensibility</em> (which suggests something about the style or content is suppressing the phrase).</p>
</div>
<div class="section level2">
<h2 id="counting-things-other-than-words">Counting things other than words<a class="anchor" aria-label="anchor" href="#counting-things-other-than-words"></a>
</h2>
<p>Text analysis is a main motivator for this implementation of weighted log odds, but this is a general approach for measuring how much more likely one feature (any kind of feature, not just a word or bigram) is to be associated than another for some set or group (any kind of set, not just a document or book).</p>
<p>To demonstrate this, let’s look at everybody’s favorite data about cars. What do we know about the relationship between number of gears and engine shape <code>vs</code>?</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">gear_counts</span> <span class="op">&lt;-</span> <span class="va">mtcars</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">vs</span>, <span class="va">gear</span><span class="op">)</span>

<span class="va">gear_counts</span>
<span class="co">#&gt;   vs gear  n</span>
<span class="co">#&gt; 1  0    3 12</span>
<span class="co">#&gt; 2  0    4  2</span>
<span class="co">#&gt; 3  0    5  4</span>
<span class="co">#&gt; 4  1    3  3</span>
<span class="co">#&gt; 5  1    4 10</span>
<span class="co">#&gt; 6  1    5  1</span></code></pre></div>
<p>Now we can use <code><a href="../reference/bind_log_odds.html">bind_log_odds()</a></code> to find the weighted log odds for each number of gears and engine shape. First, let’s use the default empirical Bayes prior. It regularizes the values.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">regularized</span> <span class="op">&lt;-</span> <span class="va">gear_counts</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="../reference/bind_log_odds.html">bind_log_odds</a></span><span class="op">(</span><span class="va">vs</span>, <span class="va">gear</span>, <span class="va">n</span><span class="op">)</span>

<span class="va">regularized</span>
<span class="co">#&gt;   vs gear  n log_odds_weighted</span>
<span class="co">#&gt; 1  0    3 12         1.1728347</span>
<span class="co">#&gt; 2  0    4  2        -1.3767516</span>
<span class="co">#&gt; 3  0    5  4         0.4033125</span>
<span class="co">#&gt; 4  1    3  3        -1.1354777</span>
<span class="co">#&gt; 5  1    4 10         1.5661168</span>
<span class="co">#&gt; 6  1    5  1        -0.4362340</span></code></pre></div>
<p>For engine shape <code>vs = 0</code>, having three gears has the highest weighted log odds while for engine shape <code>vs = 1</code>, having four gears has the highest weighted log odds. This dataset is small enough that you can look at the count data and see how this is working.</p>
<p>Now, let’s use the uninformative prior, and compare to the unweighted log odds. These log odds will be farther from zero than the regularized estimates.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">unregularized</span> <span class="op">&lt;-</span> <span class="va">gear_counts</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>
  <span class="fu"><a href="../reference/bind_log_odds.html">bind_log_odds</a></span><span class="op">(</span><span class="va">vs</span>, <span class="va">gear</span>, <span class="va">n</span>, uninformative <span class="op">=</span> <span class="cn">TRUE</span>, unweighted <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">unregularized</span>
<span class="co">#&gt;   vs gear  n   log_odds log_odds_weighted</span>
<span class="co">#&gt; 1  0    3 12  0.6968169         1.8912729</span>
<span class="co">#&gt; 2  0    4  2 -1.2527630        -1.9691060</span>
<span class="co">#&gt; 3  0    5  4  0.3249262         0.5549172</span>
<span class="co">#&gt; 4  1    3  3 -0.9673459        -1.7407107</span>
<span class="co">#&gt; 5  1    4 10  1.1451323         2.8421436</span>
<span class="co">#&gt; 6  1    5  1 -0.5268260        -0.6570674</span></code></pre></div>
<p>Most importantly, you can notice that this approach is useful both in the initial motivating example of text data but also more generally whenever you have counts in some kind of groups or sets and you want to find what feature is more likely to come from a group, compared to the other groups.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tyler Schnoebelen, Julia Silge, Alex Hayes.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.2.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
